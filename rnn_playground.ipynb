{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, DelimitedFiles                                                       \n",
    "using LinearAlgebra, Random                                                      \n",
    "using Distributions                                                              \n",
    "using Flux: onehot, onehotbatch, onecold, throttle, kldivergence, chunk, batchseq\n",
    "using Flux.Optimise: update!                                                     \n",
    "using Zygote "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zygote.@nograd onehotbatch\n",
    "\n",
    "labels = 0:1\n",
    "batch_size = 10\n",
    "\n",
    "train_path = \"tfim1D_samples\"\n",
    "psi_path = \"tfim1D_psi\"\n",
    "\n",
    "data = Int.(readdlm(train_path))\n",
    "psi = readdlm(psi_path)[:,1]\n",
    "\n",
    "train_data = data[1:10000, :]\n",
    "test_data = data[10001:end, :]\n",
    "\n",
    "N = size(train_data,2)\n",
    "\n",
    "m = Chain(\n",
    "    GRU(length(labels), 100),\n",
    "    GRU(100, length(labels)),\n",
    "    softmax\n",
    ")\n",
    "m = f64(m)\n",
    "opt = ADAM(0.01)\n",
    "\n",
    "train_data = [train_data[i,:] for i = 1:size(train_data, 1)]\n",
    "test_data = [test_data[i,:] for i = 1:size(test_data, 1)]\n",
    "\n",
    "train_data = map(ch -> onehotbatch(ch, labels), train_data)\n",
    "test_data = map(ch -> onehotbatch(ch, labels), test_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fidelity (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function probability(v)\n",
    "    Flux.reset!(m)\n",
    "    \n",
    "    if length(size(v)) == 1\n",
    "        v0 = onehot(0, labels)\n",
    "        prob = dot(m(v0), v[1][:,1])\n",
    "        \n",
    "        for i in 1:N-1\n",
    "            prob *= dot(m(v[1][:,i]), v[1][:,i+1])\n",
    "        end\n",
    "        Flux.reset!(m)\n",
    "\n",
    "    else\n",
    "        prob = zeros(size(v,1))\n",
    "        for j in 1:size(v,1)\n",
    "            v0 = onehot(0, labels)\n",
    "            \n",
    "            prob[j] = dot(m(v0), v[j][:,1])\n",
    "            for i in 1:N-1\n",
    "                prob[j] *= dot(m(v[j][:,i]), v[j][:,i+1])\n",
    "            end\n",
    "            \n",
    "            Flux.reset!(m)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return prob\n",
    "end\n",
    "\n",
    "function loss(v)\n",
    "    # Negative log-likelihood\n",
    "    if length(size(v)) == 1\n",
    "        prob = probability(v)\n",
    "        log_prob = log(prob)\n",
    "        return -log_prob\n",
    "    else\n",
    "        prob = probability(v)\n",
    "        log_prob = log.(prob)\n",
    "        return -sum(log_prob) / length(prob)\n",
    "    end\n",
    "end\n",
    "\n",
    "function generate_hilbert_space(;hot=false)\n",
    "    dim = [i for i in 0:2^N-1]\n",
    "    space = space = parse.(Int64, split(bitstring(dim[1])[end-N+1:end],\"\"))\n",
    "    \n",
    "    for i in 2:length(dim)\n",
    "        tmp = parse.(Int64, split(bitstring(dim[i])[end-N+1:end],\"\"))\n",
    "        space = hcat(space, tmp)\n",
    "    end\n",
    "    \n",
    "    space = transpose(space)\n",
    "\n",
    "    if hot\n",
    "        space = [space[i,:] for i = 1:size(space, 1)]\n",
    "        space = map(ch -> onehotbatch(ch, labels), space) \n",
    "        space = reshape(space, (length(dim),1))\n",
    "    end\n",
    "    \n",
    "    return space\n",
    "end\n",
    "\n",
    "function fidelity(space, target)\n",
    "    return dot(target, sqrt.(probability(space)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fidelity: 0.8978442343132055\n",
      "loss: 2.9472771669840547\n"
     ]
    }
   ],
   "source": [
    "space = generate_hilbert_space(hot=true)\n",
    "ps = Flux.params(m)\n",
    "\n",
    "train_data = [view(train_data, k:k+batch_size-1, :) for k in 1:batch_size:size(train_data,1)]; \n",
    "test_data = [view(test_data, k:k+batch_size-1, :) for k in 1:batch_size:size(test_data,1)]; \n",
    "                                                                                 \n",
    "epochs = 1:100                                                                   \n",
    "num_batches = size(train_data,1) # needs to generalize                           \n",
    "num_samples = 2000\n",
    "\n",
    "println(\"fidelity: \", fidelity(space, psi))\n",
    "println(\"loss: \", loss(test_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep = 1\n",
      "loss: 2.9472771669840547\n",
      "fidelity: 0.8978442343132055\n",
      "ep = 2\n",
      "loss: 2.9472771669840547\n",
      "fidelity: 0.8978442343132055\n",
      "ep = 3\n",
      "loss: 2.9472771669840547\n",
      "fidelity: 0.8978442343132055\n",
      "ep = 4\n",
      "loss: 2.9472771669840547\n",
      "fidelity: 0.8978442343132055\n",
      "ep = 5\n",
      "loss: 2.9472771669840547\n",
      "fidelity: 0.8978442343132055\n",
      "ep = 6\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] Array at ./boot.jl:406 [inlined]",
      " [2] Array at ./boot.jl:415 [inlined]",
      " [3] Array at ./boot.jl:423 [inlined]",
      " [4] similar at ./abstractarray.jl:675 [inlined]",
      " [5] similar at ./abstractarray.jl:674 [inlined]",
      " [6] similar at ./broadcast.jl:197 [inlined]",
      " [7] similar at ./broadcast.jl:196 [inlined]",
      " [8] copy at ./broadcast.jl:862 [inlined]",
      " [9] materialize at ./broadcast.jl:837 [inlined]",
      " [10] (::Flux.GRUCell{Array{Float64,2},Array{Float64,1}})(::Array{Float64,1}, ::Flux.OneHotVector) at /Users/isaacdevlugt/.julia/packages/Flux/05b38/src/layers/recurrent.jl:165",
      " [11] (::Flux.Recur{Flux.GRUCell{Array{Float64,2},Array{Float64,1}}})(::Flux.OneHotVector) at /Users/isaacdevlugt/.julia/packages/Flux/05b38/src/layers/recurrent.jl:36",
      " [12] applychain(::Tuple{Flux.Recur{Flux.GRUCell{Array{Float64,2},Array{Float64,1}}},Flux.Recur{Flux.GRUCell{Array{Float64,2},Array{Float64,1}}},typeof(softmax)}, ::Flux.OneHotVector) at /Users/isaacdevlugt/.julia/packages/Flux/05b38/src/layers/basic.jl:36",
      " [13] (::Chain{Tuple{Flux.Recur{Flux.GRUCell{Array{Float64,2},Array{Float64,1}}},Flux.Recur{Flux.GRUCell{Array{Float64,2},Array{Float64,1}}},typeof(softmax)}})(::Flux.OneHotVector) at /Users/isaacdevlugt/.julia/packages/Flux/05b38/src/layers/basic.jl:38",
      " [14] probability(::SubArray{Flux.OneHotMatrix{Array{Flux.OneHotVector,1}},2,Array{Flux.OneHotMatrix{Array{Flux.OneHotVector,1}},2},Tuple{UnitRange{Int64},Base.Slice{Base.OneTo{Int64}}},false}) at ./In[3]:20",
      " [15] loss(::SubArray{Flux.OneHotMatrix{Array{Flux.OneHotVector,1}},2,Array{Flux.OneHotMatrix{Array{Flux.OneHotVector,1}},2},Tuple{UnitRange{Int64},Base.Slice{Base.OneTo{Int64}}},false}) at ./In[3]:37",
      " [16] top-level scope at In[5]:5",
      " [17] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "for ep in epochs\n",
    "    @show ep\n",
    "    for b in 1:num_batches\n",
    "        batch = train_data[b]\n",
    "        l = loss(batch)\n",
    "        gs = Flux.gradient(() -> l, ps)\n",
    "        # gs = Flux.gradient(() -> loss(batch), Flux.params(m))\n",
    "        # the code below doesn't work for some reason. \n",
    "        # Error: Mutating arrays is not supported\n",
    "        \n",
    "        update!(opt, ps, gs)\n",
    "    end\n",
    "\n",
    "    println(\"loss: \",loss(test_data[1]))\n",
    "    println(\"fidelity: \", fidelity(space, psi))\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
